train_args:
  optim: adamw_torch
  num_train_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  eval_strategy: "no"
  save_strategy: "steps"
  save_steps: 2000
  save_total_limit: 1
  learning_rate: 3e-4
  weight_decay: 0.
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  logging_steps: 10
#  fsdp: "full_shard auto_wrap"
#  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  bf16: TRUE
  tf32: TRUE