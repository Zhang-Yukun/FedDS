model_name: meta-llama/Llama-3.1-8B  # path to pretrained foundation llm
cache_dir: ~
model_max_length: 512